---
title: "ST793 Final Project"
author: "Emmett Kendall, Neil Dey"
date: "11/18/2021"
header-includes: 
  - \usepackage{bm} 
  - \usepackage{physics} 
output: 
  html_document
---
\newcommand{\bb}[1]{\boldsymbol{#1}}
\newcommand{\setst}{\,:\,}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\operatorname{E}}
\renewcommand{\var}{\operatorname{Var}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# _Bootstrap Methods: Another look at the Jackknife_
### B. Efron

*** 

## Introduction 
The format of this blog post will be as follows. In [Section I](#sec1) we begin with a summary and clear description of the main topics discussed in Efron's paper. In [Section II](#sec2), we will include the proofs of major theorems and remarks made. Lastly, in [Section III](#sec3), we will include illustrative examples and simulations that help the reader understand the impact and importance of Bootstrap.

## Section I: Summary of article {#sec1}
### Bootstrap Methods
The ultimate goal of the methods outlined in bootstrap are to take observations, $\bb{X}$, from an unknown distribution, $F$, and understanding the sampling distribution of a chosen random quantity, $R(\bb{X}, F)$, of interest using only the observed data you have available. In the context of this paper, there existed a method called "Jackknife" which mainly worked for two forms of the $R(\bb{X}, F)$ that is
\begin{align*}
R(\bb{X}, F) &= t(\bb{X}) - \theta(F)\\
R(\bb{X}, F) &= \frac{t(\bb{X}) - \widehat{Bias}(t) - \theta(F)}{\sqrt{\widehat{Var}(t)}}
\end{align*}
where $t(\bb{X})$ estimates $\theta(F)$. What is worth noting is that jackknife approximates the distribution of $R$ by sampling $n-1$ times \textbf{with} replacement. Bootstrap is then introduced and follows the general framwork below

1. Given the existing data $\bb{X} = (x_1,...,x_n)$, assign each data point probability $1/n$ such that selecting an observation from the sample has equal probability. 
2. Sample from $\bb{X}$ \textbf{with} replacement to get $\bb{X}^*$  which has data points ${x_1^*, ..., x_n^*} \subseteq \bb{X}$. Do this multiple times since each $\bb{X_i^*}$ will be from the sampling distribution $\hat{F}$.
3. Now we get the \textit{bootstrap distribution} which is the "empirical distribution" for $R(\bb{X},F)$, namely $R^* = R(\bb{X^*}, \hat F)$.

The whole motivation is that if the resampled distribution $\hat F$ is remotely close to the true $F$, then the estimator for $R$'s distribution will be close to the truth. A large factor in the success of this method, however, is the functional form of $R$. Additionally, while it may be relatively straightforward to calculate the expected value and variance of $R^*$, actually deriving its distribution proves to be difficult. In this paper, we will dive into the three main derivation techniques for uncovering this distribution. They are

1. Pure theoretical derivations and calculations
2. Monte Carlo simulations (get an empirical distribution for $R^*$)
3. Taylor series approximations.

### Estimating the median {#med}
Consider a simple illustration. Suppose, for the unknown distribution $F$, we are interested in uncovering the median. Call $\theta(F)$ the true median, and let $t(\bb{X}) = X_{(m)}$ be the estimator, i.e. the sample median. For ease, assume the sample size, $n$, is odd ($\exists m\in \mathbb{Z}^+$ s.t. $n=2m+1$).

### Error rate estimation in discriminant analysis {#disAn}
Suppose that we have two independent samples $x_1, \ldots, x_m \overset{iid}{\sim}F$ and $y_1, \ldots, y_n \overset{iid}{\sim} G$, where $F$ and $G$ are unknown continuous probability distributions on $\mathbb{R}^k$.
Given a new data point $z$ in $\R^k$, how might we determine if it was generated from $F$ or $G$?
One method to estimate the generating distribution of $z$ is by using our observations $\bb{x}$ and $\bb{y}$ to partition $\mathbb{R}^k$ into two regions $A$ and $B$, and estimating that $z$ was generated by $F$ if $z\in A$, and otherwise estimating that $z$ was generated by $G$. 
The error rate of assigning to the $F$ distribution is given by
\begin{equation*}
  e_F = \Pr_F(X \in B)
\end{equation*}
with empirical error rate
\begin{equation*}
  \widehat{e}_F = \qty(\frac{|\{i \,:\, x_i \in B\}|}{m})
\end{equation*}
We are interested in the difference $e_F - \widehat{e}_F$ as well as the analogous difference for $G$. 

### Relationship with the jackknife {#relJack}
We now examine the approximate bias and variance of our bootstrap estimators. We show that these approximations agree exactly with the approximations given by the \textit{infinitesimal Jackknife} method, and we also show agreement with the ordinary Jackknife up to a factor of $1 + O(1/n)$.

In a one-sample situation, define $N_i^* = |\{j \,:\, X_j^* = X_i\}|$, the number of bootstrap samples that match the $i$th observation.
Note that $\vb*{N}^* \sim \operatorname{Multinom}(n; \vb*{1}/n)$ so that $\E[\vb*{N}^*] = \vb*{1}$ and $\var[\vb*{N}^*] = \vb*{I} - \vb*{1}^\top\vb*{1}/n$. 
Then define $\vb*{P}^* = \vb*{N}^*/n$ as a ``normalized" version of $\vb*{N}^*$ that estimates probabilities of the bootstrap samples that match the $i$th observation.
Note, then, that
\begin{equation}\label{eq:Pmoments}
  \E[\vb*{P}^*] = \vb*{1}/n \quad\quad \var[\vb*{P}^*] = \vb*{I}/n^2 - \vb*{11}^\top/n^3.
\end{equation}

Let us suppose that the random variable of interest, $R(\vb*{X}, F)$, is invariant to with respect to the ordering of $X_1, \ldots, X_n$. 
That is, we have \textit{exchangability} of the observations.
With this assumption, note that we can recover both $\vb*{X}$ (up to permutation) and $\widehat{F}$ when we are given $\vb*{P}^*$.
Hence, it makes sense to consider the random variable of interest as simply a function of $\vb*{P}^*$:
\begin{equation*}
  R(\vb*{P}^*) = R(\vb*{X}^*, \widehat{F}).
\end{equation*}

In order to attain estimates of the bias and variance of $R(\vb*{P}^*)$, we want to perform a Taylor expansion.
However, note that $\vb*{P}^*$ has the property that its elements sum to $1$.
By Taylor expanding as-is, not all inputs to $R$ would respect this restriction.
Hence, we define the extension
\begin{equation*}
  R^*(\vb*{v}) = R\qty(\vb*{v}/\sum_{i=1}^n v_i)
\end{equation*}
Then $R^*$ agrees with $R$ on any possible realization of $\vb*{P}^*$, and any input to $R^*$ in the first hyperoctant will allow us to recover a valid bootstrap sample.

Thus, we Taylor expand $R^*(\vb*{P}^*)$ about $\vb*{1}/n$ (the mean of $\vb*{P}^*$) to second order:
\begin{equation*}
  R^*(\vb*{P}^*) \approx R^*(\vb*{1}/n) + DR^*(\vb*{1}/n)(\vb*{P}^*-\vb*{1}/n) + \frac{1}{2}(\vb*{P}^* - \vb*{1}/n)^\top HR^*(\vb*{1}/n)(\vb*{P}^* - \vb*{1}/n).
\end{equation*}

We now note two important properties concerning the derivatives of $R^*$: We have that
\begin{equation}\label{eq:DRx}
  DR^*(\vb*{1}/n)\vb*{1} = 0
\end{equation}
and
\begin{equation}\label{eq:HRx}
  HR^*(\vb*{1}/n)\vb*{1} = -n\cdot DR^*(\vb*{1}/n)^\top.
\end{equation}

To prove that equation \ref{eq:DRx} holds, we simply apply chain rule several times:
\begin{align*}
  DR^* &= \pdv{\vb*{P}} \qty[R\qty(\vb*{P}/\sum_{i=1}^n P_i)] \\
       &= DR\qty(\frac{\vb*{P}}{\sum P_i}) \cdot \frac{1}{(\sum P_i)^2}\qty(\vb*{I}\sum_{i=1}^n P_i - \vb*{1}^\top \otimes \vb*{P}) \\
       &= \frac{1}{(\sum P_i)^2}\mqty[\qty(\sum P_i)\cdot D_1R\qty(\frac{\vb*{P}}{\sum P_i}) - P_1\sum\limits_{j=1}^n D_j{R}\qty(\frac{\vb*{P}}{\sum P_i}) & \cdots & \qty(\sum P_i)\cdot D_nR\qty(\frac{\vb*{P}}{\sum P_i}) - P_n\sum\limits_{j=1}^n D_j{R}\qty(\frac{\vb*{P}}{\sum P_i})].
\end{align*}
Evaluating at $\vb*{1}/n$, we thus have that 
\begin{equation*}
  DR^*(\vb*{1}/n) = \mqty[D_1R(\vb*{1}/n) - \frac{1}{n}\sum\limits_{j=1}^n D_jR(\vb*{1}/n) & \cdots & D_nR(\vb*{1}/n) - \frac{1}{n}\sum\limits_{j=1}^n D_jR(\vb*{1}/n)]
\end{equation*}
and so
\begin{align*}
  DR^*(\vb*{1}/n)\vb*{1} 
  &= \sum_{i=1}^n\qty(D_iR(\vb*{1}/n) - \frac{1}{n}\sum\limits_{j=1}^n D_jR(\vb*{1}/n)) \\
  &= \sum_{i=1}^n D_iR(\vb*{1}/n) - \sum_{j=1}^n D_jR(\vb*{1}/n) \\
  &= 0
\end{align*}
as desired. We perform a similar calculation to verify equation \ref{eq:HRx}. We first calculate the $k$th row of the Hessian matrix for $R^*$:
\begin{align*}
  H_kR^*(\vb*{P}) &= D(D_kR^*) \\
  &= D\qty(\frac{1}{\sum P_i}\cdot DR\qty(\frac{\vb*{P}}{\sum P_i})\vb*{e}_k - \frac{P_k}{(\sum P_i)^2}DR\qty(\frac{\vb*{P}}{\sum P_i})\cdot \vb*{1}) \\
  &= -\frac{\vb*{1}^\top}{(\sum P_i)^2}D_kR\qty(\frac{\vb*{P}}{\sum P_i}) + \qty(\frac{2P_k\vb*{1}^\top}{(\sum P_i)^3} - \frac{\vb*{e}_k^\top}{(\sum P_i)^2})\sum_{i=1}^n D_iR\qty(\frac{\vb*{P}}{\sum P_i}) + \qty(\frac{\vb*{e}_k^\top}{(\sum P_i)^3} - \frac{P_k - \vb*{1}^\top}{(\sum P_i)^4})HR\qty(\frac{\vb*{P}}{\sum P_i})\qty(\vb*{I}\sum P_i - \vb*{1}^\top \otimes \vb*{P}).
\end{align*}
Evaluating at $\vb*{1}/n$, we have that
\begin{equation*}
  H_kR^*(\vb*{1}/n) = -\vb*{1}^\top D_k(\vb*{1}/n) + \qty(\frac{2}{n}\vb*{1} - \vb*{e}_k^\top)\sum_{i=1}^n D_iR(\vb*{1}/n) + \qty(\vb*{e_k^\top} - \vb*{1}/n)HR(\vb*{1}/n)(\vb*{I} - \vb*{1}^\top \otimes \vb*{1}/n)
\end{equation*}
Summing the elements of the $k$th row, we find that
\begin{align*}
  H_kR^*(\vb*{1}/n)\vb*{1} &= -nD_kR(\vb*{1}/n) + \sum_{i=1}^n D_iR(\vb*{1}/n) + 0 \\
    &= -n D_kR^*(\vb*{1}/n)
\end{align*}
as desired, as the term involving the Hessian matrix of $R$ is zero by a tedious but straightforward calculation. 

Having established these facts, we can now calculate the expectation and variance of $R(\vb*{P}^*)$. For the expectation, we have that 
\begin{align*}
  \E[R^*(\vb*{P}^*)]
  &\approx R^*(\vb*{1}/n) + DR^*(\vb*{1}/n)\E\qty[(\vb*{P}^*-\vb*{1}/n)] + \frac{1}{2}\E\qty[(\vb*{P}^* - \vb*{1}/n)^\top HR^*(\vb*{1}/n)(\vb*{P}^* - \vb*{1}/n)] \\
  &= R^*(\vb*{1}/n) + DR^*(\vb*{1}/n) \cdot 0 + \frac{1}{2}\qty[\tr(HR^*(\vb*{1}/n) \cdot \var[\vb*{P}^*]) + \vb*{0}^\top HR^*(\vb*{1}/n)\vb*{0}] \\
  &= R^*(\vb*{1}/n) + \frac{1}{2}\tr(HR^*(\vb*{1}/n)(\vb*{I}/n^2 - \vb*{11}^\top/n)) \\
  &= R^*(\vb*{1}/n) + \frac{1}{2n^2}\tr(HR^*(\vb*{1}/n)) - \frac{1}{2}\tr(-nDR^*(\vb*{1}/n)^\top\vb*{1}^\top/n^3) \\
  &= R^*(\vb*{1}/n) + \frac{1}{2n^2}\tr(HR^*(\vb*{1}/n)) - \frac{1}{2}\tr(0) \\
  &= R^*(\vb*{1}/n) + \frac{1}{2n}\overline{\vb*{V}}
\end{align*}
where $\overline{\vb*{V}} = \frac{1}{n}\sum H_{ii}R^*(\vb*{1}/n)$. Note the use of equation \ref{eq:Pmoments} in the second and third lines above, as well as equation \ref{eq:HRx} in the fourth line and equation \ref{eq:DRx} in the fifth. Hence, our bias is approximately
\begin{equation} \label{eq:bootbias}
  \operatorname{Bias}[R(\vb*{X}^*), \widehat{F})] \approx \frac{1}{2n} \overline{\vb*{V}}.
\end{equation}

Similarly with the variance (this time only Taylor expanding to first order), we have that
\begin{align*}
  \var[R^*(\vb*{P}^*)] 
  &\approx \var[R^*(\vb*{1}/n) + DR^*(\vb*{1}/n)(\vb*{P}^*-\vb*{1}/n)] \\
  &= DR^*(\vb*{1}/n)\var[\vb*{P}^*-\vb*{1}/n]DR^*(\vb*{1}/n)^\top \\
  &= DR^*(\vb*{1}/n)\qty(\vb*{I}/n^2 + \vb*{11}^\top/n^3)DR^*(\vb*{1}/n)^\top
\end{align*}
which simplifies to
\begin{equation} \label{eq:bootvar}
  \var[R(\vb*{X}^*, \widehat{F})] \approx \frac{1}{n^2}\sum_{i=1}^n D_iR^*(\vb*{1}/n)^2
\end{equation}

The above expressions for the bias and variance are exactly those given by the infinitesimal Jackknife. We can also compare these expressions to those given by the usual Jackknife, which replaces partial derivatives $D_iR^*$ with the finite differences
\begin{equation*}
  \Delta_iR^* = (n-1)\qty[\frac{1}{n}\sum_{j=1}^n R^*\qty(\frac{\vb*{1}-\vb*{e}_j}{n-1}) - R^*\qty(\frac{\vb*{1}-\vb*{e}_i}{n-1})],
\end{equation*}
where $\vb*{e}_j$ is the $j$th standard basis vector.
Note that the above definition of $\Delta_iR^*$ is simply a direct implementation of the Jackknife's ``leave one out" procedure, but written in the language of Bootstrap.
In this way, the bias and variance variance estimates from the ordinary Jackknife are instead given by 
\begin{align}
  \E[R(\vb*{X}^*, \widehat{F})] &\approx \frac{1}{2(n-1)}\overline{\vb*{V}} \label{eq:jbias} \\
  \var[R(\vb*{X}^*, \widehat{F})] &\approx \frac{1}{n(n-1)}\sum_{i=1}^n \qty(\Delta_iR^*)^2. \label{eq:jvar}
\end{align}

The bootstrap bias estimate of equation \ref{eq:bootbias} differs from the Jackknife bias estimate of \ref{eq:jbias} by a factor of $n/(n-1) = 1 + O(1/n)$. 
We can show that the variance estimates of the bootstrap (eq. \ref{eq:bootvar}) and Jackknife (eq. \ref{eq:jvar}) differ by the same factor of $1 + O(1/n)$. 

Via the Taylor expansion, we have that
\begin{align*}
  R^*\qty(\frac{\vb*{1}-\vb*{e}_i}{n-1})
  &= R^*\qty(\vb*{1}/n) +  DR^*(\vb*{1}/n)\qty(\frac{\vb*{1}}{n(n-1)} - \frac{\vb*{e}_i}{n-1}) + \frac{1}{2}\qty(\frac{\vb*{1}}{n(n-1)} - \frac{\vb*{e}_i}{n-1})^\top HR^*(\vb*{1}/n)\qty(\frac{\vb*{1}}{n(n-1)} - \frac{\vb*{e}_i}{n-1}), \\
\intertext{which using equations \ref{eq:DRx} and \ref{eq:HRx} simplifies to}
  &= R^*\qty(\vb*{1}/n) - \frac{D_iR^*(\vb*{1}/n)}{n-1} + \frac{D_iR^*(\vb*{1}/n)}{(n-1)^2} + \frac{H_{ii}R^*(\vb*{1}/n)}{2(n-1)^2} \\
  &= R^*\qty(\vb*{1}/n) - \frac{n-2}{(n-1)^2} D_iR^*(\vb*{1}/n)+ \frac{H_{ii}R^*(\vb*{1}/n)}{2(n-1)^2}.
\end{align*}
We thus see that
\begin{align*}
  \Delta_iR^* 
  &= (n-1)\qty[\frac{1}{n}\sum_{j=1}^n R^*\qty(\frac{\vb*{1}-\vb*{e}_j}{n-1}) - R^*\qty(\frac{\vb*{1}-\vb*{e}_i}{n-1})] \\
  &= (n-1)\qty[\frac{1}{n}\sum_{j=1}^n\qty[R^*\qty(\vb*{1}/n) - \frac{n-2}{(n-1)^2} D_jR^*(\vb*{1}/n)+ \frac{H_{jj}R^*(\vb*{1}/n)}{2(n-1)^2}] - R^*\qty(\vb*{1}/n) + \frac{n-2}{(n-1)^2} D_iR^*(\vb*{1}/n) - \frac{H_{ii}R^*(\vb*{1}/n)}{2(n-1)^2}] \\
  &= \frac{1}{2n(n-1)}\sum_{j=1}^n H_{jj}R^*(\vb*{1}/n) + \frac{n-2}{n-1}D_iR^*(\vb*{1}/n) - \frac{H_{ii}R^*(\vb*{1}/n)}{2(n-1)} \\
  &= \frac{n-2}{n-1}D_iR^*(\vb*{1}/n) - \frac{1}{2(n-1)}\qty(H_{ii}R^*(\vb*{1}/n) - \overline{\vb*{V}})
\end{align*}
Dividing through by $D_iR^*(\vb*{1}/n)$ again yields that $\Delta_iR^*/D_iR^*(\vb*{1}/n) = 1 + O(1/n)$, immediately yielding the same factor for the ratio of the variance estimates.

### Wilcoxon's statistic {#wilStat}

### Regression models {#reg}

## Section II: Theory {#sec2}
<!-- we will use this section sort of as an appendix. Put all proofs here with a link and then reference them from the summary-->


## Section III: Examples {#sec3}
<!-- these will be our personal examples that are the ones that (i guess) will count for our grade -->

### Example 1 {#ex1}

### Example 2 {#ex2}

