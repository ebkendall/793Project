---
title: "ST793 Final Project"
author: "Emmett Kendall, Neil Dey"
date: "11/18/2021"
header-includes: 
  - \usepackage{bm} 
  - \usepackage{physics} 
output: 
  html_document
---
\newcommand{\bb}[1]{\boldsymbol{#1}}
\newcommand{\setst}{\,:\,}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\operatorname{E}}
\renewcommand{\var}{\operatorname{Var}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# _Bootstrap Methods: Another look at the Jackknife_
### B. Efron

*** 

## Introduction 
The format of this blog post will be as follows. In [Section I](#sec1) we begin with a summary and clear description of the main topics discussed in Efron's paper. In [Section II](#sec2), we will include the proofs of major theorems and remarks made. Lastly, in [Section III](#sec3), we will include illustrative examples and simulations that help the reader understand the impact and importance of Bootstrap.

## Section I: Summary of article {#sec1}
### Bootstrap Methods
The ultimate goal of the methods outlined in bootstrap are to take observations, $\bb{X}$, from an unknown distribution, $F$, and understanding the sampling distribution of a chosen random quantity, $R(\bb{X}, F)$, of interest using only the observed data you have available. In the context of this paper, there existed a method called "Jackknife" which mainly worked for two forms of the $R(\bb{X}, F)$ that is
\begin{align*}
R(\bb{X}, F) &= t(\bb{X}) - \theta(F)\\
R(\bb{X}, F) &= \frac{t(\bb{X}) - \widehat{Bias}(t) - \theta(F)}{\sqrt{\widehat{Var}(t)}}
\end{align*}
where $t(\bb{X})$ estimates $\theta(F)$. What is worth noting is that jackknife approximates the distribution of $R$ by sampling $n-1$ times \textbf{with} replacement. Bootstrap is then introduced and follows the general framwork below

1. Given the existing data $\bb{X} = (x_1,...,x_n)$, assign each data point probability $1/n$ such that selecting an observation from the sample has equal probability. 
2. Sample from $\bb{X}$ \textbf{with} replacement to get $\bb{X}^*$  which has data points ${x_1^*, ..., x_n^*} \subseteq \bb{X}$. Do this multiple times since each $\bb{X_i^*}$ will be from the sampling distribution $\hat{F}$.
3. Now we get the \textit{bootstrap distribution} which is the "empirical distribution" for $R(\bb{X},F)$, namely $R^* = R(\bb{X^*}, \hat F)$.

The whole motivation is that if the resampled distribution $\hat F$ is remotely close to the true $F$, then the estimator for $R$'s distribution will be close to the truth. A large factor in the success of this method, however, is the functional form of $R$. Additionally, while it may be relatively straightforward to calculate the expected value and variance of $R^*$, actually deriving its distribution proves to be difficult. In this paper, we will dive into the three main derivation techniques for uncovering this distribution. They are

1. Pure theoretical derivations and calculations
2. Monte Carlo simulations (get an empirical distribution for $R^*$)
3. Taylor series approximations.

### Estimating the median {#med}
Consider a simple illustration. Suppose, for the unknown distribution $F$, we are interested in uncovering the median. Call $\theta(F)$ the true median, and let $t(\bb{X}) = X_{(m)}$ be the estimator, i.e. the sample median. For ease, assume the sample size, $n$, is odd ($\exists m\in \mathbb{Z}^+$ s.t. $n=2m+1$).

### Error rate estimation in discriminant analysis {#disAn}
Suppose that we have two independent samples $x_1, \ldots, x_m \overset{iid}{\sim}F$ and $y_1, \ldots, y_n \overset{iid}{\sim} G$, where $F$ and $G$ are unknown continuous probability distributions on $\mathbb{R}^k$.
Given a new data point $z$ in $\R^k$, how might we determine if it was generated from $F$ or $G$?
One method to estimate the generating distribution of $z$ is by using our observations $\bb{x}$ and $\bb{y}$ to partition $\mathbb{R}^k$ into two regions $A$ and $B$, and estimating that $z$ was generated by $F$ if $z\in A$, and otherwise estimating that $z$ was generated by $G$. 
The error rate of assigning to the $F$ distribution is given by
\begin{equation*}
  e_F = \Pr_F(X \in B)
\end{equation*}
with empirical error rate
\begin{equation*}
  \widehat{e}_F = \qty(\frac{|\{i \,:\, x_i \in B\}|}{m})
\end{equation*}
We are interested in the difference $e_F - \widehat{e}_F$ as well as the analogous difference for $G$. 

### Relationship with the jackknife {#relJack}
We now examine the approximate bias and variance of our bootstrap estimators to show that they agree with the approximations given by the \textit{infinitesimal Jackknife} method.
In a one-sample situation, define $N_i^* = |\{j \,:\, X_j^* = X_i\}|$, the number of bootstrap samples that match the $i$th observation.
Note that $\vb*{N}^* \sim \operatorname{Multinom}(n; \vb*{1}/n)$ so that $\E[\vb*{N}^*] = \vb*{1}$ and $\var[\vb*{N}^*] = \vb*{I} - \vb*{1}^\top\vb*{1}/n$. 
Then define $\vb*{P}^* = \vb*{N}^*/n$ as a ``normalized" version of $\vb*{N}^*$ that estimates probabilities of the bootstrap samples that match the $i$th observation.
Note, then, that
\begin{equation}
  \E[\vb*{P}^*] = \vb*{1}/n \quad\quad \var[\vb*{P}^*] = \vb*{I}/n^2 - \vb*{1}^\top\vb*{1}/n^3.
\end{equation}

Let us suppose that the random variable of interest, $R(\vb*{X}, F)$, is invariant to with respect to the ordering of $X_1, \ldots, X_n$. 
That is, we have \textit{exchangability} of the observations.
With this assumption, note that we can recover both $\vb*{X}$ (up to permutation) and $\widehat{F}$ when we are given $\vb*{P}^*$.
Hence, it makes sense to consider the random variable of interest as simply a function of $\vb*{P}^*$:
\begin{equation*}
  R(\vb*{P}^*) = R(\vb*{X}^*, \widehat{F}).
\end{equation*}

In order to attain estimates of the bias and variance of $R(\vb*{P}^*)$, we want to perform a Taylor expansion.
However, note that $\vb*{P}^*$ has the property that its elements sum to $1$.
By Taylor expanding as-is, not all inputs to $R$ would respect this restriction.
Hence, we define the extension
\begin{equation*}
  R^*(\vb*{v}) = R\qty(\vb*{v}/\sum_{i=1}^n v_i)
\end{equation*}
Then $R^*$ agrees with $R$ on any possible realization of $\vb*{P}^*$, and any input to $R^*$ in the first hyperoctant will allow us to recover a valid bootstrap sample.

Thus, we Taylor expand $R^*$ about the point $\vb*{1}/n$, ignoring what we assume to be a negligible remainder term:
\begin{equation*}
  R^*(\vb*{P}) = R^*(\vb*{1}/n) + DR^*(\vb*{1}/n)(\vb*{P}^*-\vb*{1}/n) + \frac{1}{2}(\vb*{P}^* - \vb*{1}/n)^\top HR^*(\vb*{1}/n)(\vb*{P}^* - \vb*{1}/n).
\end{equation*}

We now note two important properties concerning the derivatives of $R^*$: We have that
\begin{equation}\label{eq:DRx}
  DR^*(\vb*{1}/n)\vb*{1} = 0
\end{equation}
and
\begin{equation}\label{eq:HRx}
  HR^*(\vb*{1}/n)\vb*{1} = -n\cdot DR^*(\vb*{1}/n).
\end{equation}

To prove that equation \ref{eq:DRx} holds, we simply apply chain rule several times:
\begin{align*}
  DR^* &= \pdv{\vb*{P}} \qty[R\qty(\vb*{P}/\sum_{i=1}^n P_i)] \\
       &= DR\qty(\frac{\vb*{P}}{\sum P_i}) \cdot \frac{1}{(\sum P_i)^2}\qty(\vb*{I}\sum_{i=1}^n P_i - \vb*{1}^\top \otimes \vb*{P}) \\
       &= \frac{1}{(\sum P_i)^2}\mqty[\qty(\sum P_i)\cdot D_1R\qty(\frac{\vb*{P}}{\sum P_i}) - P_1\sum\limits_{j=1}^n D_j{R}\qty(\frac{\vb*{P}}{\sum P_i}) & \cdots & \qty(\sum P_i)\cdot D_nR\qty(\frac{\vb*{P}}{\sum P_i}) - P_n\sum\limits_{j=1}^n D_j{R}\qty(\frac{\vb*{P}}{\sum P_i})].
\end{align*}
Evaluating at $\vb*{1}/n$, we thus have that 
\begin{equation*}
  DR^*(\vb*{1}/n) = \mqty[D_1R(\vb*{1}/n) - \frac{1}{n}\sum\limits_{j=1}^n D_jR(\vb*{1}/n) & \cdots & D_nR(\vb*{1}/n) - \frac{1}{n}\sum\limits_{j=1}^n D_jR(\vb*{1}/n)]
\end{equation*}
and so
\begin{align*}
  DR^*(\vb*{1}/n)\vb*{1} 
  &= \sum_{i=1}^n\qty(D_iR(\vb*{1}/n) - \frac{1}{n}\sum\limits_{j=1}^n D_jR(\vb*{1}/n)) \\
  &= \sum_{i=1}^n D_iR(\vb*{1}/n) - \sum_{j=1}^n D_jR(\vb*{1}/n) \\
  &= 0
\end{align*}
as desired. We perform a similar calculation to verify equation \ref{eq:HRx}:
\begin{align*}
  H_{l,k}R^* 
  &= D_l\qty(\frac{1}{\sum P_i}\cdot D_kR\qty(\frac{\vb*{P}}{\sum P_i}) - \frac{P_k}{\qty(\sum P_i)^2} \cdot \sum\limits_{j=1}^n D_j{R}\qty(\frac{\vb*{P}}{\sum P_i})) \\
  &= -\frac{D_kR\qty(\frac{\vb*{P}}{\sum P_i})}{\qty(\sum P_i)^2}+ \frac{-P_l + \sum P_i}{\qty(\sum P_i)^3}H_{l, k}R\qty(\frac{\vb*{P}}{\sum P_i}) - \frac{P_k}{\qty(\sum P_i)^4}\sum_{j=1}^n H_{j,k}R\qty(\frac{\vb*{P}}{\sum P_i}) \cdot \qty(-P_l + \sum_{i=1}^n P_i) - \frac{2P_k - I(l = k)\cdot\sum P_i}{\qty(\sum P_i)^3} \cdot \sum_{j=1}^n D_jR\qty(\frac{\vb*{P}}{\sum P_i})
\end{align*}
Evaluating at $\vb*{1}/n$, we have that
\begin{equation*}
  H_{l,k}R^*(\vb*{1}/n) = -D_kR(\vb*{1}/n) + \frac{n-1}{n}H_{l,k}R(\vb*{1}/n) - \frac{n-1}{n^2}\sum_{j=1}^n H_{l, j}R(\vb*{1}/n) + \qty(\frac{2}{n} - I(l = k))\sum_{j=1}^n D_jR(\vb*{1}/n)
\end{equation*}
and so
\begin{align*}
  \sum_{l=1}^n H_{l,k}R^*(\vb*{1}/n) 
  &= -n\cdot D_kR(\vb*{1}/n) - \frac{n-1}{n^2}\sum_{l=1}^n \sum_{j=1}^n H_{l, j}R(\vb*{1}/n) + \frac{n-1}{n^2}\sum_{l=1}^n \sum_{j=1}^n H_{l, j}R(\vb*{1}/n) + \sum_{j=1}^n D_jR(\vb*{1}/n) \\
  &= -n \cdot D_kR^*(\vb*{1}/n) - \frac{n-1}{n^2}\sum_{l=1}^n \sum_{j=1}^n H_{l, j}R(\vb*{1}/n) + \frac{n-1}{n^2}\sum_{l=1}^n \sum_{j=1}^n H_{l, j}R(\vb*{1}/n)
\end{align*}
which isn't quite what we want because those second derivatives need to cancel out :(

### Wilcoxon's statistic {#wilStat}

### Regression models {#reg}

## Section II: Theory {#sec2}
<!-- we will use this section sort of as an appendix. Put all proofs here with a link and then reference them from the summary-->


## Section III: Examples {#sec3}
<!-- these will be our personal examples that are the ones that (i guess) will count for our grade -->

### Example 1 {#ex1}

### Example 2 {#ex2}

