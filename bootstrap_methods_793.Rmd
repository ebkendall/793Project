---
title: "ST793 Final Project"
author: "Emmett Kendall, Neil Dey"
date: "11/18/2021"
header-includes: 
  - \usepackage{bm} 
output: 
  html_document
---
\newcommand{\bb}[1]{\boldsymbol{#1}}
\newcommand{\setst}{\,:\,}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\operatorname{E}}
\renewcommand{\var}{\operatorname{Var}}
\newcommand{\tr}{\operatorname{tr}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# _Bootstrap Methods: Another look at the Jackknife_
### B. Efron

*** 

## Introduction 
The format of this blog post will be as follows. In [Section I](#sec1) we begin with a summary and clear description of the main topics discussed in Efron's paper. In [Section II](#sec2), we will include the proofs of major theorems and remarks made. Lastly, in [Section III](#sec3), we will include illustrative examples and simulations that help the reader understand the impact and importance of Bootstrap.

## Section I: Summary of article {#sec1}
### Bootstrap Methods
The ultimate goal of the methods outlined in bootstrap are to take observations, $\bb{X}$, from an unknown distribution, $F$, and understanding the sampling distribution of a chosen random quantity, $R(\bb{X}, F)$, of interest using only the observed data you have available. In the context of this paper, there existed a method called "Jackknife" which mainly worked for two forms of the $R(\bb{X}, F)$ that is
\begin{align*}
R(\bb{X}, F) &= t(\bb{X}) - \theta(F)\\
R(\bb{X}, F) &= \frac{t(\bb{X}) - \widehat{Bias}(t) - \theta(F)}{\sqrt{\widehat{Var}(t)}}
\end{align*}
where $t(\bb{X})$ estimates $\theta(F)$. What is worth noting is that jackknife approximates the distribution of $R$ by sampling $n-1$ times \textbf{with} replacement. Bootstrap is then introduced and follows the general framwork below

1. Given the existing data $\bb{X} = (x_1,...,x_n)$, assign each data point probability $1/n$ such that selecting an observation from the sample has equal probability. 
2. Sample from $\bb{X}$ \textbf{with} replacement to get $\bb{X}^*$  which has data points ${x_1^*, ..., x_n^*} \subseteq \bb{X}$. Do this multiple times since each $\bb{X_i^*}$ will be from the sampling distribution $\hat{F}$.
3. Now we get the \textit{bootstrap distribution} which is the "empirical distribution" for $R(\bb{X},F)$, namely $R^* = R(\bb{X^*}, \hat F)$.

The whole motivation is that if the resampled distribution $\hat F$ is remotely close to the true $F$, then the estimator for $R$'s distribution will be close to the truth. A large factor in the success of this method, however, is the functional form of $R$. Additionally, while it may be relatively straightforward to calculate the expected value and variance of $R^*$, actually deriving its distribution proves to be difficult. In this paper, we will dive into the three main derivation techniques for uncovering this distribution. They are

1. Pure theoretical derivations and calculations
2. Monte Carlo simulations (get an empirical distribution for $R^*$)
3. Taylor series approximations.

### Estimating the median {#med}
Consider a simple illustration. Suppose, for the unknown distribution $F$, we are interested in uncovering the median. Call $\theta(F)$ the true median, and let $t(\bb{X}) = X_{(m)}$ be the estimator, i.e. the sample median. For ease, assume the sample size, $n$, is odd ($\exists m\in \mathbb{Z}^+$ s.t. $n=2m+1$).

### Relationship with the jackknife {#relJack}
We now examine the approximate bias and variance of our bootstrap estimators. We show that these approximations agree exactly with the approximations given by the \textit{infinitesimal Jackknife} method, and we also show agreement with the ordinary Jackknife up to a factor of $1 + O(1/n)$.

In a one-sample situation, define $N_i^* = |\{j \,:\, X_j^* = X_i\}|$, the number of bootstrap samples that match the $i$th observation.
Note that $\bb{N}^* \sim \operatorname{Multinom}(n; \bb{1}/n)$ so that $\E[\bb{N}^*] = \bb{1}$ and $\var[\bb{N}^*] = \bb{I} - \bb{1}^\top\bb{1}/n$. 
Then define $\bb{P}^* = \bb{N}^*/n$ as a ``normalized" version of $\bb{N}^*$ that estimates probabilities of the bootstrap samples that match the $i$th observation.
Note, then, that
\begin{equation}
  \E[\bb{P}^*] = \bb{1}/n \quad\quad \var[\bb{P}^*] = \bb{I}/n^2 - \bb{11}^\top/n^3.
\end{equation}

Let us suppose that the random variable of interest, $R(\bb{X}, F)$, is invariant to with respect to the ordering of $X_1, \ldots, X_n$. 
That is, we have \textit{exchangability} of the observations.
With this assumption, note that we can recover both $\bb{X}$ (up to permutation) and $\widehat{F}$ when we are given $\bb{P}^*$.
Hence, it makes sense to consider the random variable of interest as simply a function of $\bb{P}^*$:
\begin{equation*}
  R(\bb{P}^*) = R(\bb{X}^*, \widehat{F}).
\end{equation*}

In order to attain estimates of the bias and variance of $R(\bb{P}^*)$, we want to perform a Taylor expansion.
However, note that $\bb{P}^*$ has the property that its elements sum to $1$.
By Taylor expanding as-is, not all inputs to $R$ would respect this restriction.
Hence, we define the extension
\begin{equation*}
  R^*(\bb{v}) = R\left(\bb{v}/\sum_{i=1}^n v_i\right)
\end{equation*}
Then $R^*$ agrees with $R$ on any possible realization of $\bb{P}^*$, and any input to $R^*$ in the first hyperoctant will allow us to recover a valid bootstrap sample.

Thus, we Taylor expand $R^*(\bb{P}^*)$ about $\bb{1}/n$ (the mean of $\bb{P}^*$) to second order:
\begin{equation*}
  R^*(\bb{P}^*) \approx R^*(\bb{1}/n) + DR^*(\bb{1}/n)(\bb{P}^*-\bb{1}/n) + \frac{1}{2}(\bb{P}^* - \bb{1}/n)^\top HR^*(\bb{1}/n)(\bb{P}^* - \bb{1}/n).
\end{equation*}

We now note two important properties concerning the derivatives of $R^*$: We have that
\begin{equation}
  DR^*(\bb{1}/n)\bb{1} = 0
\end{equation}
and
\begin{equation}\label{eq:HRx}
  HR^*(\bb{1}/n)\bb{1} = -n\cdot DR^*(\bb{1}/n)^\top.
\end{equation}

To prove that $DR^*(\bb{1}/n)\bb{1} = 0$, we simply apply chain rule several times:
\begin{align*}
  DR^* &= D \left[R\left(\bb{P}/\sum_{i=1}^n P_i\right)\right] \\
       &= DR\left(\frac{\bb{P}}{\sum P_i}\right) \cdot \frac{1}{(\sum P_i)^2}\left(\bb{I}\sum_{i=1}^n P_i - \bb{1}^\top \otimes \bb{P}\right) \\
       &= \frac{1}{(\sum P_i)^2}\begin{bmatrix}(\sum P_i)\cdot D_1R\left(\frac{\bb{P}}{\sum P_i}\right) - P_1\sum\limits_{j=1}^n D_j{R}\left(\frac{\bb{P}}{\sum P_i}\right) & \cdots & (\sum P_i)\cdot D_nR\left(\frac{\bb{P}}{\sum P_i}\right) - P_n\sum\limits_{j=1}^n D_j{R}\left(\frac{\bb{P}}{\sum P_i}\right)\end{bmatrix}.
\end{align*}
Evaluating at $\bb{1}/n$, we thus have that 
\begin{equation*}
  DR^*(\bb{1}/n) = \begin{bmatrix}D_1R(\bb{1}/n) - \frac{1}{n}\sum\limits_{j=1}^n D_jR(\bb{1}/n) & \cdots & D_nR(\bb{1}/n) - \frac{1}{n}\sum\limits_{j=1}^n D_jR(\bb{1}/n)\end{bmatrix}
\end{equation*}
and so
\begin{align*}
  DR^*(\bb{1}/n)\bb{1} 
  &= \sum_{i=1}^n\left(D_iR(\bb{1}/n) - \frac{1}{n}\sum\limits_{j=1}^n D_jR(\bb{1}/n)\right) \\
  &= \sum_{i=1}^n D_iR(\bb{1}/n) - \sum_{j=1}^n D_jR(\bb{1}/n) \\
  &= 0
\end{align*}
as desired. We perform a similar calculation to verify that $HR^*(\bb{1}/n)\bb{1} = -n\cdot DR^*(\bb{1}/n)^\top$. We first calculate the $k$th row of the Hessian matrix for $R^*$:
\begin{align*}
  H_kR^*(\bb{P}) &= D(D_kR^*) \\
  &= D\left(\frac{1}{\sum P_i}\cdot DR\left(\frac{\bb{P}}{\sum P_i}\right)\bb{e}_k - \frac{P_k}{(\sum P_i)^2}DR\left(\frac{\bb{P}}{\sum P_i}\right)\cdot \bb{1}\right) \\
  &= -\frac{\bb{1}^\top}{(\sum P_i)^2}D_kR\left(\frac{\bb{P}}{\sum P_i}\right) + \left(\frac{2P_k\bb{1}^\top}{(\sum P_i)^3} - \frac{\bb{e}_k^\top}{(\sum P_i)^2}\right)\sum_{i=1}^n D_iR\left(\frac{\bb{P}}{\sum P_i}\right) + \left(\frac{\bb{e}_k^\top}{(\sum P_i)^3} - \frac{P_k - \bb{1}^\top}{(\sum P_i)^4}\right)HR\left(\frac{\bb{P}}{\sum P_i}\right)\left(\bb{I}\sum P_i - \bb{1}^\top \otimes \bb{P}\right).
\end{align*}
Evaluating at $\bb{1}/n$, we have that
\begin{equation*}
  H_kR^*(\bb{1}/n) = -\bb{1}^\top D_k(\bb{1}/n) + \left(\frac{2}{n}\bb{1} - \bb{e}_k^\top\right)\sum_{i=1}^n D_iR(\bb{1}/n) + \left(\bb{e_k^\top} - \bb{1}/n\right)HR(\bb{1}/n)(\bb{I} - \bb{1}^\top \otimes \bb{1}/n)
\end{equation*}
Summing the elements of the $k$th row, we find that
\begin{align*}
  H_kR^*(\bb{1}/n)\bb{1} &= -nD_kR(\bb{1}/n) + \sum_{i=1}^n D_iR(\bb{1}/n) + 0 \\
    &= -n D_kR^*(\bb{1}/n)
\end{align*}
as desired, as the term involving the Hessian matrix of $R$ is zero by a tedious but straightforward calculation. 

Having established these facts, we can now calculate the expectation and variance of $R(\bb{P}^*)$. For the expectation, we have that 
\begin{align*}
  \E[R^*(\bb{P}^*)]
  &\approx R^*(\bb{1}/n) + DR^*(\bb{1}/n)\E\left[(\bb{P}^*-\bb{1}/n)\right] + \frac{1}{2}\E\left[(\bb{P}^* - \bb{1}/n)^\top HR^*(\bb{1}/n)(\bb{P}^* - \bb{1}/n)\right] \\
  &= R^*(\bb{1}/n) + DR^*(\bb{1}/n) \cdot 0 + \frac{1}{2}\left[\tr(HR^*(\bb{1}/n) \cdot \var[\bb{P}^*]) + \bb{0}^\top HR^*(\bb{1}/n)\bb{0}\right] \\
  &= R^*(\bb{1}/n) + \frac{1}{2}\tr\left[HR^*(\bb{1}/n)(\bb{I}/n^2 - \bb{11}^\top/n)\right] \\
  &= R^*(\bb{1}/n) + \frac{1}{2n^2}\tr(HR^*(\bb{1}/n)) - \frac{1}{2}\tr\left[-nDR^*(\bb{1}/n)^\top\bb{1}^\top/n^3\right] \\
  &= R^*(\bb{1}/n) + \frac{1}{2n^2}\tr(HR^*(\bb{1}/n)) - \frac{1}{2}\tr(0) \\
  &= R^*(\bb{1}/n) + \frac{1}{2n}\overline{\bb{V}}
\end{align*}
where $\overline{\bb{V}} = \frac{1}{n}\sum H_{ii}R^*(\bb{1}/n)$. Note the use of $\E[\bb{P}^*] = \bb{1}/n$ and \var[\bb{P}^*] = \bb{I}/n^2 - \bb{11}^\top/n^3 in the second and third lines above, as well as the fact that $HR^*(\bb{1}/n)\bb{1} = -n\cdot DR^*(\bb{1}/n)^\top$ in the fourth line and that $DR^*(\bb{1}/n)\bb{1} = 0$ in the fifth. Hence, our bias is approximately
\begin{equation} \label{eq:bootbias}
  \operatorname{Bias}[R(\bb{X}^*), \widehat{F})] \approx \frac{1}{2n} \overline{\bb{V}}.
\end{equation}

Similarly with the variance (this time only Taylor expanding to first order), we have that
\begin{align*}
  \var[R^*(\bb{P}^*)] 
  &\approx \var[R^*(\bb{1}/n) + DR^*(\bb{1}/n)(\bb{P}^*-\bb{1}/n)] \\
  &= DR^*(\bb{1}/n)\var[\bb{P}^*-\bb{1}/n]DR^*(\bb{1}/n)^\top \\
  &= DR^*(\bb{1}/n)(\bb{I}/n^2 + \bb{11}^\top/n^3)DR^*(\bb{1}/n)^\top
\end{align*}
which (using the fact that $DR^*(\bb*{1}/n)\bb*{1} = 0$) simplifies to
\begin{equation} \label{eq:bootvar}
  \var[R(\bb{X}^*, \widehat{F})] \approx \frac{1}{n^2}\sum_{i=1}^n D_iR^*(\bb{1}/n)^2
\end{equation}

The above expressions for the bias and variance are exactly those given by the infinitesimal Jackknife. We can also compare these expressions to those given by the usual Jackknife, which replaces partial derivatives $D_iR^*$ with the finite differences
\begin{equation*}
  \Delta_iR^* = (n-1)\left[\frac{1}{n}\sum_{j=1}^n R^*\left(\frac{\bb{1}-\bb{e}_j}{n-1}\right) - R^*\left(\frac{\bb{1}-\bb{e}_i}{n-1}\right)\right],
\end{equation*}
where $\bb{e}_j$ is the $j$th standard basis vector.
Note that the above definition of $\Delta_iR^*$ is simply a direct implementation of the Jackknife's ``leave one out" procedure, but written in the language of Bootstrap.
In this way, the bias and variance variance estimates from the ordinary Jackknife are instead given by 
\begin{align}
  \E[R(\bb{X}^*, \widehat{F})] &\approx \frac{1}{2(n-1)}\overline{\bb{V}} \label{eq:jbias} \\
  \var[R(\bb{X}^*, \widehat{F})] &\approx \frac{1}{n(n-1)}\sum_{i=1}^n \left(\Delta_iR^*\right)^2. \label{eq:jvar}
\end{align}

We see that the bootstrap bias estimate differs from the Jackknife bias estimate by a factor of $n/(n-1) = 1 + O(1/n)$. 

We can show that the variance estimates of the bootstrap and Jackknifediffer by the same factor of $1 + O(1/n)$. Via the Taylor expansion, we have that
\begin{align*}
  R^*\left(\frac{\bb{1}-\bb{e}_i}{n-1}\right)
  &= R^*(\bb{1}/n) +  DR^*(\bb{1}/n)\left(\frac{\bb{1}}{n(n-1)} - \frac{\bb{e}_i}{n-1}\right) + \frac{1}{2}\left(\frac{\bb{1}}{n(n-1)} - \frac{\bb{e}_i}{n-1}\right)^\top HR^*(\bb{1}/n)\left(\frac{\bb{1}}{n(n-1)} - \frac{\bb{e}_i}{n-1}\right) \\
  &= R^*(\bb{1}/n) - \frac{D_iR^*(\bb{1}/n)}{n-1} + \frac{D_iR^*(\bb{1}/n)}{(n-1)^2} + \frac{H_{ii}R^*(\bb{1}/n)}{2(n-1)^2} \\
  &= R^*(\bb{1}/n) - \frac{n-2}{(n-1)^2} D_iR^*(\bb{1}/n)+ \frac{H_{ii}R^*(\bb{1}/n)}{2(n-1)^2}.
\end{align*}
We thus see that
\begin{align*}
  \Delta_iR^* 
  &= (n-1)\left[\frac{1}{n}\sum_{j=1}^n R^*\left(\frac{\bb{1}-\bb{e}_j}{n-1}\right) - R^*\left(\frac{\bb{1}-\bb{e}_i}{n-1}\right)\right] \\
  &= (n-1)\left[\frac{1}{n}\sum_{j=1}^n\left[R^*(\bb{1}/n) - \frac{n-2}{(n-1)^2} D_jR^*(\bb{1}/n)+ \frac{H_{jj}R^*(\bb{1}/n)}{2(n-1)^2}\right] - R^*(\bb{1}/n) + \frac{n-2}{(n-1)^2} D_iR^*(\bb{1}/n) - \frac{H_{ii}R^*(\bb{1}/n)}{2(n-1)^2}\right] \\
  &= \frac{1}{2n(n-1)}\sum_{j=1}^n H_{jj}R^*(\bb{1}/n) + \frac{n-2}{n-1}D_iR^*(\bb{1}/n) - \frac{H_{ii}R^*(\bb{1}/n)}{2(n-1)} \\
  &= \frac{n-2}{n-1}D_iR^*(\bb{1}/n) - \frac{1}{2(n-1)}(H_{ii}R^*(\bb{1}/n) - \overline{\bb{V}})
\end{align*}
Dividing through by $D_iR^*(\bb{1}/n)$ again yields that $\Delta_iR^*/D_iR^*(\bb{1}/n) = 1 + O(1/n)$, immediately yielding the same factor for the ratio of the variance estimates.

### Regression models {#reg}
Perhaps one of the most common uses of statistics is fitting linear regression models. Consider the simple linear model $\bb{Y} = \bb{X}\beta + \varepsilon$ where $\bb{Y} \in \R^n$, $\beta \in \R$, and $\varepsilon \sim N(\bb{0}, \sigma^2\bb{I})$ for unknown residual variance $\sigma^2$. Then we know that the least-squares estimator $\widehat{\beta}$ has variance $\sigma^2(\bb{X}^\top\bb{X})^{-1}$. 

We can also obtain an estimate of $\var[\widehat{\beta}]$ via bootstrapping. Given the sample $y_1, \ldots, y_n$, we can construct the residuals $\widehat{\varepsilon}_i = y_i - X_i\widehat{\beta}$. We define $\widehat{F}$ to be the empirical distribution of these residuals. We can then construct bootstrap samples $Y_i^* = X_i\widehat{\beta} + \varepsilon_i^*$ where $\varepsilon_i^* \overset{iid}{\sim} \widehat{F}$. This then lets us construct a bootstrap estimate $\widehat{\beta}^*$ of $\beta$. 
```{r}
set.seed(0)

beta <- 5
sigma <- 10

variance_diff <- c()
ns <- 2:1000
for(n in ns){
  X <- matrix(rnorm(n), nrow=n)
  y <- X * beta + rnorm(n, sd=sigma)
  
  betahat <- solve(t(X)%*%X)%*%t(X)%*%y
  
  true_variance = sigma^2 * solve(t(X)%*%X)
  
  eps_hat <- y - X%*%betahat
  
  N <- 1000
  bootstrap_betas <- c()
  for(i in 1:N){
    y_boot <- X%*%betahat + sample(eps_hat, 1)
    bootstrap_betas <- c(bootstrap_betas, solve(t(X)%*%X)%*%t(X)%*%y_boot)
  }
  variance_diff <- c(variance_diff, (var(bootstrap_betas)-true_variance)/true_variance)
}
plot(ns, variance_diff, xlab="n", ylab="Relative Error", main="Relative Error in Variance Estimate vs Sample Size")

quantile(variance_diff, 0.5)
quantile(variance_diff, 2/3)
mean(variance_diff)
```
From the above simulation, we find that bootstrap tends to underestimate the variance (with 2/3 of the bootstrap variance estimates being underestimations). However, when it does overestimate, we can see that it often severely overestimates---with several estimates even being off by a factor of 6 or more. It turns out, then, that the bootstrap variance estimates are close the the true variance. Furthermore, increasing the sample size doesn't appear to have much of an impact on the relative error in the variance estimate.

From this, one advantage of bootstrap becomes clear: If the distribution of the residual variance is unknown, the nonparametric bootstrap still allows us to obtain estimates for the variance of $\widehat{\beta}$.

We can repeat the above simulation, but this time with heteroskedasticity, in which the residual variance depends on the data $\bb{X}$, and a non-normal distribution:


## Section II: Theory {#sec2}
<!-- we will use this section sort of as an appendix. Put all proofs here with a link and then reference them from the summary-->


## Section III: Examples {#sec3}
<!-- these will be our personal examples that are the ones that (i guess) will count for our grade -->

### Example 1 {#ex1}

### Example 2 {#ex2}

